{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-10T19:29:56.642701Z","iopub.status.busy":"2024-04-10T19:29:56.642349Z","iopub.status.idle":"2024-04-10T19:29:56.647499Z","shell.execute_reply":"2024-04-10T19:29:56.646531Z","shell.execute_reply.started":"2024-04-10T19:29:56.642673Z"},"trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:29:59.479681Z","iopub.status.busy":"2024-04-10T19:29:59.479104Z","iopub.status.idle":"2024-04-10T19:30:12.571148Z","shell.execute_reply":"2024-04-10T19:30:12.570056Z","shell.execute_reply.started":"2024-04-10T19:29:59.479650Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-warmup\n","  Downloading pytorch_warmup-0.1.1-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-warmup) (2.1.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (2024.2.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1->pytorch-warmup) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1->pytorch-warmup) (1.3.0)\n","Downloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\n","Installing collected packages: pytorch-warmup\n","Successfully installed pytorch-warmup-0.1.1\n"]}],"source":["!pip install pytorch-warmup"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:30:12.573401Z","iopub.status.busy":"2024-04-10T19:30:12.573102Z","iopub.status.idle":"2024-04-10T19:30:28.710637Z","shell.execute_reply":"2024-04-10T19:30:28.709778Z","shell.execute_reply.started":"2024-04-10T19:30:12.573374Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-10 19:30:19.857963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-10 19:30:19.858060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-10 19:30:20.002924: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import math, torch, torchaudio\n","import soundfile\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import os, numpy\n","from sklearn import metrics\n","from operator import itemgetter\n","import glob, numpy, os, random, soundfile, torch\n","from scipy import signal\n","import time\n","import tqdm\n","import pytorch_warmup as warmup\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"markdown","metadata":{},"source":["**Cloning the repository**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:30:28.712651Z","iopub.status.busy":"2024-04-10T19:30:28.712125Z","iopub.status.idle":"2024-04-10T19:30:31.393280Z","shell.execute_reply":"2024-04-10T19:30:31.392156Z","shell.execute_reply.started":"2024-04-10T19:30:28.712626Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'IndicSUPERB'...\n","remote: Enumerating objects: 606, done.\u001b[K\n","remote: Counting objects: 100% (606/606), done.\u001b[K\n","remote: Compressing objects: 100% (445/445), done.\u001b[K\n","remote: Total 606 (delta 147), reused 601 (delta 145), pack-reused 0\u001b[K\n","Receiving objects: 100% (606/606), 4.37 MiB | 6.29 MiB/s, done.\n","Resolving deltas: 100% (147/147), done.\n"]}],"source":["# Clone the repository\n","!git clone https://github.com/m23csa007/IndicSUPERB.git"]},{"cell_type":"markdown","metadata":{},"source":["**Data Preprocessing**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:30:31.395769Z","iopub.status.busy":"2024-04-10T19:30:31.395042Z","iopub.status.idle":"2024-04-10T19:33:14.596730Z","shell.execute_reply":"2024-04-10T19:33:14.595546Z","shell.execute_reply.started":"2024-04-10T19:30:31.395730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying Transcripts\n","0it [00:00, ?it/s]\n","100%|███████████████████████████████████████| 1767/1767 [02:30<00:00, 11.71it/s]\n"]}],"source":["!python /kaggle/working/IndicSUPERB/utilities/structure.py \\\n","  /kaggle/input/kathbath-dataset-test-known/kb_data_clean_m4a \\\n","  /kaggle/working/kb_data_clean_wav \\\n","  malayalam"]},{"cell_type":"markdown","metadata":{},"source":["**Dataset**\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:14.599524Z","iopub.status.busy":"2024-04-10T19:33:14.599213Z","iopub.status.idle":"2024-04-10T19:33:14.615324Z","shell.execute_reply":"2024-04-10T19:33:14.614513Z","shell.execute_reply.started":"2024-04-10T19:33:14.599494Z"},"trusted":true},"outputs":[],"source":["class test_loader(object):\n","    def __init__(self, train_list, train_path, musan_path, rir_path, num_frames, **kwargs):\n","        self.train_path = train_path\n","        self.num_frames = num_frames\n","        self.data_list  = []\n","        self.data_label = []\n","        lines = open(train_list).read().splitlines()\n","        dictkeys = list(set([x.split()[0] for x in lines]))\n","        dictkeys.sort()\n","        dictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n","        for index, line in enumerate(lines):\n","            speaker_label = dictkeys[line.split()[0]]\n","            file_names = line.split()[1:]  # Get the file names from the line\n","            file_paths = []\n","            for file_name in file_names:\n","                full_file_path = os.path.join(train_path, file_name)\n","                file_paths.append(full_file_path)\n","            self.data_label.append(speaker_label)\n","            self.data_list.append(file_paths)  # Append both file paths\n","\n","    def __getitem__(self, index):\n","        try:\n","            # Read the utterance and randomly select the segment\n","            audio1, sr1 = soundfile.read(self.data_list[index][0])        \n","            length = self.num_frames * 160 + 240\n","            if audio1.shape[0] <= length:\n","                shortage = length - audio1.shape[0]\n","                audio1 = numpy.pad(audio1, (0, shortage), 'wrap')\n","            start_frame = numpy.int64(random.random()*(audio1.shape[0]-length))\n","            audio1 = audio1[start_frame:start_frame + length]\n","            #audio1 = numpy.stack([audio1], axis=0).astype(numpy.float32)\n","\n","            # Read the second utterance and randomly select the segment\n","            audio2, sr2 = soundfile.read(self.data_list[index][1])        \n","            if audio2.shape[0] <= length:\n","                shortage = length - audio2.shape[0]\n","                audio2 = numpy.pad(audio2, (0, shortage), 'wrap')\n","            start_frame = numpy.int64(random.random()*(audio2.shape[0]-length))\n","            audio2 = audio2[start_frame:start_frame + length]\n","            #audio2 = numpy.stack([audio2], axis=0).astype(numpy.float32)\n","\n","            # Concatenate the two audio files along a new dimension\n","            concatenated_audio = numpy.stack([audio1, audio2], axis=0).astype(numpy.float32)\n","            # Convert concatenated_audio and label to tensors\n","           \n","            return torch.tensor(concatenated_audio),(self.data_label[index])\n","        \n","        except Exception as e:\n","#         # If an error occurs (e.g., file not found), print a warning and return None\n","#         print(f\"Error processing file {self.data_list[index]}: {e}\")\n","            length = self.num_frames * 160 + 240\n","            return torch.zeros((2, length)), self.data_label[index]\n","        \n","\n","    def __len__(self):\n","        return len(self.data_list)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:14.616573Z","iopub.status.busy":"2024-04-10T19:33:14.616333Z","iopub.status.idle":"2024-04-10T19:33:15.103747Z","shell.execute_reply":"2024-04-10T19:33:15.102869Z","shell.execute_reply.started":"2024-04-10T19:33:14.616552Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["import torch\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import roc_curve\n","\n","# Check if CUDA is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","\n","testloader = test_loader(\"/kaggle/input/kathbath-metadata/meta_data/malayalam/test_known_data.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\n","testLoader = DataLoader(testloader, batch_size=128, shuffle=False, num_workers=10) "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:15.105332Z","iopub.status.busy":"2024-04-10T19:33:15.105024Z","iopub.status.idle":"2024-04-10T19:33:15.111327Z","shell.execute_reply":"2024-04-10T19:33:15.110372Z","shell.execute_reply.started":"2024-04-10T19:33:15.105307Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of files in testloader: 50000\n"]}],"source":["num_files = len(testloader)\n","print(\"Number of files in testloader:\", num_files)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:15.113755Z","iopub.status.busy":"2024-04-10T19:33:15.112319Z","iopub.status.idle":"2024-04-10T19:33:15.186277Z","shell.execute_reply":"2024-04-10T19:33:15.185458Z","shell.execute_reply.started":"2024-04-10T19:33:15.113727Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration: 0\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 0\n","Iteration: 1\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n","Iteration: 2\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n","Iteration: 3\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 0\n","Iteration: 4\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n","Iteration: 5\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n","Iteration: 6\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 0\n","Iteration: 7\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n","Iteration: 8\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 0\n","Iteration: 9\n","inputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","Target: 1\n"]}],"source":["#testloader = test_loader(\"/kaggle/input/voxceleb-h/voxceleb_h.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\n","iteration_count = 0\n","for batch in testloader:\n","    inputs = batch[0]  # Assuming inputs are at index 0\n","    target = batch[1]  # Assuming targets are at index 1\n","    if inputs is None or target is None:\n","        continue\n","    # Your code here\n","\n","   \n","    print(\"Iteration:\", iteration_count)\n","    iteration_count += 1\n","    # Your remaining code here\n","\n","    print(\"inputs:\", inputs)\n","    print(\"Target:\", target)\n","    \n","    if iteration_count == 10:\n","        break\n","    \n","# Input: tensor([ 0.0028,  0.0027,  0.0027,  ..., -0.0038, -0.0066, -0.0106])\n","# Input shape: torch.Size([48240])\n","# Target: 1"]},{"cell_type":"markdown","metadata":{},"source":["**Import the model**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:15.188158Z","iopub.status.busy":"2024-04-10T19:33:15.187542Z","iopub.status.idle":"2024-04-10T19:33:48.340834Z","shell.execute_reply":"2024-04-10T19:33:48.339942Z","shell.execute_reply.started":"2024-04-10T19:33:15.188125Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e64d3462bab3446b97b8112f7d46faf7","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c391c3c14bc045e48880431ca2432a6c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/58.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23ab0fe85db941bb8b21b57b143dac34","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of the model checkpoint at microsoft/wavlm-base-sv were not used when initializing WavLMForXVector: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n","- This IS expected if you are initializing WavLMForXVector from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing WavLMForXVector from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of WavLMForXVector were not initialized from the model checkpoint at microsoft/wavlm-base-sv and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["WavLMForXVector(\n","  (wavlm): WavLMModel(\n","    (feature_extractor): WavLMFeatureEncoder(\n","      (conv_layers): ModuleList(\n","        (0): WavLMGroupNormConvLayer(\n","          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n","          (activation): GELUActivation()\n","          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n","        )\n","        (1-4): 4 x WavLMNoLayerNormConvLayer(\n","          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n","          (activation): GELUActivation()\n","        )\n","        (5-6): 2 x WavLMNoLayerNormConvLayer(\n","          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n","          (activation): GELUActivation()\n","        )\n","      )\n","    )\n","    (feature_projection): WavLMFeatureProjection(\n","      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","      (projection): Linear(in_features=512, out_features=768, bias=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): WavLMEncoder(\n","      (pos_conv_embed): WavLMPositionalConvEmbedding(\n","        (conv): ParametrizedConv1d(\n","          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n","          (parametrizations): ModuleDict(\n","            (weight): ParametrizationList(\n","              (0): _WeightNorm()\n","            )\n","          )\n","        )\n","        (padding): WavLMSamePadLayer()\n","        (activation): GELUActivation()\n","      )\n","      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (layers): ModuleList(\n","        (0): WavLMEncoderLayer(\n","          (attention): WavLMAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n","            (rel_attn_embed): Embedding(320, 12)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (feed_forward): WavLMFeedForward(\n","            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n","            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (output_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1-11): 11 x WavLMEncoderLayer(\n","          (attention): WavLMAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n","          )\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (feed_forward): WavLMFeedForward(\n","            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n","            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (output_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (projector): Linear(in_features=768, out_features=512, bias=True)\n","  (tdnn): ModuleList(\n","    (0): TDNNLayer(\n","      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n","      (activation): ReLU()\n","    )\n","    (1-2): 2 x TDNNLayer(\n","      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n","      (activation): ReLU()\n","    )\n","    (3): TDNNLayer(\n","      (kernel): Linear(in_features=512, out_features=512, bias=True)\n","      (activation): ReLU()\n","    )\n","    (4): TDNNLayer(\n","      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n","      (activation): ReLU()\n","    )\n","  )\n","  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n","  (classifier): Linear(in_features=512, out_features=512, bias=True)\n","  (objective): AMSoftmaxLoss(\n","    (loss): CrossEntropyLoss()\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import numpy as np\n","from sklearn.metrics import roc_curve\n","\n","from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n","\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\n","model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n","\n","\n","# Move model and feature extractor to CUDA if available\n","model.to(device)\n","\n","# Save the pre-trained model to the specified directory\n","#model.save_pretrained(pretrained_model_directory)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-10T19:33:48.342277Z","iopub.status.busy":"2024-04-10T19:33:48.341999Z","iopub.status.idle":"2024-04-10T19:45:59.377775Z","shell.execute_reply":"2024-04-10T19:45:59.376572Z","shell.execute_reply.started":"2024-04-10T19:33:48.342252Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/tmp/ipykernel_34/2034815900.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  audio1 = torch.tensor(audio1)\n","/tmp/ipykernel_34/2034815900.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  audio2 = torch.tensor(audio2)\n"]},{"name":"stdout","output_type":"stream","text":["Equal Error Rate (EER): 50.00%\n"]}],"source":["import torch\n","import numpy as np\n","from sklearn.metrics import roc_curve\n","\n","with torch.no_grad():\n","    similarities = []\n","    labels = []\n","    for inputs, target in testLoader:\n","        #print(\"Inputs:\", inputs)\n","        #print(\"Target:\", target)\n","        if inputs is None or target is None:\n","            continue\n","            \n","        # Extract audio1 and audio2 from inputs\n","        audio1 = inputs[:, 0]  \n","        audio2 = inputs[:, 1]  \n","        \n","        # Pass the input tensors to the model\n","        audio1 = torch.tensor(audio1)\n","        audio2 = torch.tensor(audio2)\n","        #print(\"audio1 shape\", audio1.shape)\n","        \n","        # Extract embeddings\n","        audio1 = feature_extractor(audio1, return_tensors=\"pt\", sampling_rate=16000)\n","        audio1 = audio1.input_values.squeeze(0) \n","       \n","        audio2 = feature_extractor(audio2, return_tensors=\"pt\", sampling_rate=16000)\n","        audio2 = audio2.input_values.squeeze(0) \n","        \n","        audio1 = audio1.clone().detach().to(device)\n","        audio2 = audio2.clone().detach().to(device)\n","        \n","        # Get embeddings for audio1\n","        #print(\"audio1 shape after embedding extraction\", audio1.shape)\n","        outputs_audio1 = model(input_values=audio1)\n","        embeddings_audio1 = outputs_audio1.embeddings\n","        embeddings_audio1 = torch.nn.functional.normalize(embeddings_audio1, dim=-1).cpu()\n","        \n","        # Get embeddings for audio2\n","        outputs_audio2 = model(input_values=audio2)\n","        embeddings_audio2 = outputs_audio2.embeddings\n","        embeddings_audio2 = torch.nn.functional.normalize(embeddings_audio2, dim=-1).cpu()\n","        \n","        # Calculate cosine similarity between embeddings\n","        similarity = torch.nn.functional.cosine_similarity(embeddings_audio1, embeddings_audio2, dim=-1)\n","        \n","#         print(\"Similarity:\", similarity)\n","#         print(\"Label:\", target)\n","        \n","        similarities.extend(similarity.tolist())\n","        labels.extend(target.tolist())  # Append the tensor itself, not its item\n","\n","# Convert lists to numpy arrays\n","similarities = np.array(similarities)\n","labels = np.array(labels)\n","\n","# Calculate ROC curve\n","fpr, tpr, thresholds = roc_curve(labels, similarities, pos_label=1)\n","\n","# Find EER\n","eer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\n","eer = (fpr[np.argmin(np.abs(fpr - (1 - tpr)))] + (1 - tpr)[np.argmin(np.abs(fpr - (1 - tpr)))]) / 2\n","\n","print(\"Equal Error Rate (EER): {:.2f}%\".format(eer * 100))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4742975,"sourceId":8044106,"sourceType":"datasetVersion"},{"datasetId":4772090,"sourceId":8084350,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
