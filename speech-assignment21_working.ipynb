{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2816656,"sourceType":"datasetVersion","datasetId":1722209},{"sourceId":8028065,"sourceType":"datasetVersion","datasetId":4731558}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T15:08:05.572405Z","iopub.execute_input":"2024-04-05T15:08:05.572917Z","iopub.status.idle":"2024-04-05T15:08:05.577856Z","shell.execute_reply.started":"2024-04-05T15:08:05.572889Z","shell.execute_reply":"2024-04-05T15:08:05.576966Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:08:23.995427Z","iopub.execute_input":"2024-04-05T15:08:23.996221Z","iopub.status.idle":"2024-04-05T15:08:37.412476Z","shell.execute_reply.started":"2024-04-05T15:08:23.996191Z","shell.execute_reply":"2024-04-05T15:08:37.411571Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pytorch-warmup\n  Downloading pytorch_warmup-0.1.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-warmup) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (2024.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1->pytorch-warmup) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1->pytorch-warmup) (1.3.0)\nDownloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\nInstalling collected packages: pytorch-warmup\nSuccessfully installed pytorch-warmup-0.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import math, torch, torchaudio\nimport soundfile\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os, numpy\nfrom sklearn import metrics\nfrom operator import itemgetter\nimport glob, numpy, os, random, soundfile, torch\nfrom scipy import signal\nimport time\nimport tqdm\nimport pytorch_warmup as warmup\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:08:44.994084Z","iopub.execute_input":"2024-04-05T15:08:44.994963Z","iopub.status.idle":"2024-04-05T15:09:02.726785Z","shell.execute_reply.started":"2024-04-05T15:08:44.994927Z","shell.execute_reply":"2024-04-05T15:09:02.725863Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-05 15:08:52.549946: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-05 15:08:52.550058: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-05 15:08:52.677382: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"class test_loader(object):\n    def __init__(self, train_list, train_path, musan_path, rir_path, num_frames, **kwargs):\n        self.train_path = train_path\n        self.num_frames = num_frames\n        # Load and configure augmentation files\n#         self.noisetypes = ['noise','speech','music']\n#         self.noisesnr = {'noise':[0,15],'speech':[13,20],'music':[5,15]}\n#         self.numnoise = {'noise':[1,1], 'speech':[3,8], 'music':[1,1]}\n#         self.noiselist = {}\n#         augment_files   = glob.glob(os.path.join(musan_path,'*/*/*/*.wav'))\n#         for file in augment_files:\n#             if file.split('/')[-4] not in self.noiselist:\n#                 self.noiselist[file.split('/')[-4]] = []\n#             self.noiselist[file.split('/')[-4]].append(file)\n#         self.rir_files  = glob.glob(os.path.join(rir_path,'*/*/*.wav'))\n        # Load data & labels\n        self.data_list  = []\n        self.data_label = []\n        lines = open(train_list).read().splitlines()\n        dictkeys = list(set([x.split()[0] for x in lines]))\n        dictkeys.sort()\n        dictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n        for index, line in enumerate(lines):\n            speaker_label = dictkeys[line.split()[0]]\n            file_names = line.split()[1:]  # Get the file names from the line\n            file_paths = []\n            for file_name in file_names:\n                full_file_path = os.path.join(train_path, file_name)\n                file_paths.append(full_file_path)\n            self.data_label.append(speaker_label)\n            self.data_list.append(file_paths)  # Append both file paths\n\n    def __getitem__(self, index):\n        try:\n            # Read the utterance and randomly select the segment\n            audio1, sr1 = soundfile.read(self.data_list[index][0])        \n            length = self.num_frames * 160 + 240\n            if audio1.shape[0] <= length:\n                shortage = length - audio1.shape[0]\n                audio1 = numpy.pad(audio1, (0, shortage), 'wrap')\n            start_frame = numpy.int64(random.random()*(audio1.shape[0]-length))\n            audio1 = audio1[start_frame:start_frame + length]\n            #audio1 = numpy.stack([audio1], axis=0).astype(numpy.float32)\n\n            # Read the second utterance and randomly select the segment\n            audio2, sr2 = soundfile.read(self.data_list[index][1])        \n            if audio2.shape[0] <= length:\n                shortage = length - audio2.shape[0]\n                audio2 = numpy.pad(audio2, (0, shortage), 'wrap')\n            start_frame = numpy.int64(random.random()*(audio2.shape[0]-length))\n            audio2 = audio2[start_frame:start_frame + length]\n            #audio2 = numpy.stack([audio2], axis=0).astype(numpy.float32)\n\n            # Data Augmentation\n    #         augtype = random.randint(0,5)\n    #         if augtype == 0:   # Original\n    #             audio = audio\n    #         elif augtype == 1: # Reverberation\n    #             audio = self.add_rev(audio)\n    #         elif augtype == 2: # Babble\n    #             audio = self.add_noise(audio, 'speech')\n    #         elif augtype == 3: # Music\n    #             audio = self.add_noise(audio, 'music')\n    #         elif augtype == 4: # Noise\n    #             audio = self.add_noise(audio, 'noise')\n    #         elif augtype == 5: # Television noise\n    #             audio = self.add_noise(audio, 'speech')\n    #             audio = self.add_noise(audio, 'music')\n            # Return the two audio files as tensors along with the label\n\n            # Concatenate the two audio files along a new dimension\n            concatenated_audio = numpy.stack([audio1, audio2], axis=0).astype(numpy.float32)\n\n            return concatenated_audio, self.data_label[index]\n        \n        except Exception as e:\n#         # If an error occurs (e.g., file not found), print a warning and return None\n#         print(f\"Error processing file {self.data_list[index]}: {e}\")\n            return torch.zeros((2, length)), self.data_label[index]\n        \n\n    def __len__(self):\n        return len(self.data_list)\n\n    def add_rev(self, audio):\n        rir_file    = random.choice(self.rir_files)\n        rir, sr     = soundfile.read(rir_file)\n        rir         = numpy.expand_dims(rir.astype(numpy.float32),0)\n        rir         = rir / numpy.sqrt(numpy.sum(rir**2))\n        return signal.convolve(audio, rir, mode='full')[:,:self.num_frames * 160 + 240]\n\n    def add_noise(self, audio, noisecat):\n        clean_db    = 10 * numpy.log10(numpy.mean(audio ** 2)+1e-4) \n        numnoise    = self.numnoise[noisecat]\n        noiselist   = random.sample(self.noiselist[noisecat], random.randint(numnoise[0],numnoise[1]))\n        noises = []\n        for noise in noiselist:\n            noiseaudio, sr = soundfile.read(noise)\n            length = self.num_frames * 160 + 240\n            if noiseaudio.shape[0] <= length:\n                shortage = length - noiseaudio.shape[0]\n                noiseaudio = numpy.pad(noiseaudio, (0, shortage), 'wrap')\n            start_frame = numpy.int64(random.random()*(noiseaudio.shape[0]-length))\n            noiseaudio = noiseaudio[start_frame:start_frame + length]\n            noiseaudio = numpy.stack([noiseaudio],axis=0)\n            noise_db = 10 * numpy.log10(numpy.mean(noiseaudio ** 2)+1e-4) \n            noisesnr   = random.uniform(self.noisesnr[noisecat][0],self.noisesnr[noisecat][1])\n            noises.append(numpy.sqrt(10 ** ((clean_db - noise_db - noisesnr) / 10)) * noiseaudio)\n        noise = numpy.sum(numpy.concatenate(noises,axis=0),axis=0,keepdims=True)\n        return noise + audio\n    \n    def collate_fn(self, batch):\n        # Filter out None values from the batch\n        batch = [b for b in batch if b is not None]\n        # Convert non-None values to tensors using torch.as_tensor()\n        return [torch.as_tensor(b) for b in batch]","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:09:23.104044Z","iopub.execute_input":"2024-04-05T15:09:23.104741Z","iopub.status.idle":"2024-04-05T15:09:23.133010Z","shell.execute_reply.started":"2024-04-05T15:09:23.104703Z","shell.execute_reply":"2024-04-05T15:09:23.131949Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_curve\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\ntestloader = test_loader(\"/kaggle/input/voxceleb-h/voxceleb_h.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\ntestLoader = DataLoader(testloader, batch_size=64, shuffle=False, num_workers=10)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:11:03.343908Z","iopub.execute_input":"2024-04-05T15:11:03.344808Z","iopub.status.idle":"2024-04-05T15:11:07.584875Z","shell.execute_reply.started":"2024-04-05T15:11:03.344774Z","shell.execute_reply":"2024-04-05T15:11:07.583865Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"# testloader = test_loader(\"/kaggle/input/voxceleb-h/voxceleb_h.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\n# iteration_count = 0\n# for inputs, target in testloader:\n#     if inputs is None or target is None:\n#         continue\n#     # Your code here\n\n   \n#     print(\"Iteration:\", iteration_count)\n#     iteration_count += 1\n#     # Your remaining code here\n\n#     print(\"inputs:\", inputs.shape)\n#     print(\"Target:\", target)\n    \n#     if iteration_count == 10:\n#         break\n    \n# # Input: tensor([ 0.0028,  0.0027,  0.0027,  ..., -0.0038, -0.0066, -0.0106])\n# # Input shape: torch.Size([48240])\n# # Target: 1","metadata":{"execution":{"iopub.status.busy":"2024-04-05T12:14:46.906013Z","iopub.status.idle":"2024-04-05T12:14:46.906349Z","shell.execute_reply.started":"2024-04-05T12:14:46.906167Z","shell.execute_reply":"2024-04-05T12:14:46.906179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_files = len(testloader)\nprint(\"Number of files in testloader:\", num_files)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:09:39.008894Z","iopub.execute_input":"2024-04-05T15:09:39.009260Z","iopub.status.idle":"2024-04-05T15:09:39.014467Z","shell.execute_reply.started":"2024-04-05T15:09:39.009231Z","shell.execute_reply":"2024-04-05T15:09:39.013435Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of files in testloader: 552536\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Print the data loader\n# for batch_idx, (inputs, targets) in enumerate(testLoader):\n#     print(\"Batch\", batch_idx)\n#     print(\"Inputs:\", inputs)\n#     print(\"Targets:\", targets)\n#     break  # Print only the first batch","metadata":{"execution":{"iopub.status.busy":"2024-04-05T12:14:46.909215Z","iopub.status.idle":"2024-04-05T12:14:46.909574Z","shell.execute_reply.started":"2024-04-05T12:14:46.909414Z","shell.execute_reply":"2024-04-05T12:14:46.909428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Print the data loader\n# for batch_idx, (inputs, targets) in enumerate(testLoader):\n#     print(\"Batch\", batch_idx)\n#     print(\"Inputs:\", inputs)\n#     print(\"Targets:\", targets)\n#     break  # Print only the first batch","metadata":{"execution":{"iopub.status.busy":"2024-04-05T12:14:46.910905Z","iopub.status.idle":"2024-04-05T12:14:46.911204Z","shell.execute_reply.started":"2024-04-05T12:14:46.911055Z","shell.execute_reply":"2024-04-05T12:14:46.911067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Model finetuned on voxceleb1 dataset\n\n#microsoft/unispeech-sat-base-sv\n#microsoft/wavlm-base-sv\n#microsoft/wavlm-base-plus-sv","metadata":{"execution":{"iopub.status.busy":"2024-04-05T12:14:46.912048Z","iopub.status.idle":"2024-04-05T12:14:46.912400Z","shell.execute_reply.started":"2024-04-05T12:14:46.912207Z","shell.execute_reply":"2024-04-05T12:14:46.912227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_curve\n\n# Load the feature extractor and model\nfrom transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\nmodel = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n\n# Move model and feature extractor to CUDA if available\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:09:44.042089Z","iopub.execute_input":"2024-04-05T15:09:44.042824Z","iopub.status.idle":"2024-04-05T15:09:52.138872Z","shell.execute_reply.started":"2024-04-05T15:09:44.042793Z","shell.execute_reply":"2024-04-05T15:09:52.138003Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fef4483999b4d45b46f4213383f6f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/58.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93be61c6b3464aa1a94c4c0ef997684a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1a7dde17ac492c8d284dc76245fdb8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at microsoft/wavlm-base-sv were not used when initializing WavLMForXVector: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing WavLMForXVector from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing WavLMForXVector from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of WavLMForXVector were not initialized from the model checkpoint at microsoft/wavlm-base-sv and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"WavLMForXVector(\n  (wavlm): WavLMModel(\n    (feature_extractor): WavLMFeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): WavLMGroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1-4): 4 x WavLMNoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x WavLMNoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): WavLMFeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): WavLMEncoder(\n      (pos_conv_embed): WavLMPositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): WavLMSamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0): WavLMEncoderLayer(\n          (attention): WavLMAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n            (rel_attn_embed): Embedding(320, 12)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): WavLMFeedForward(\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-11): 11 x WavLMEncoderLayer(\n          (attention): WavLMAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): WavLMFeedForward(\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (projector): Linear(in_features=768, out_features=512, bias=True)\n  (tdnn): ModuleList(\n    (0): TDNNLayer(\n      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (1-2): 2 x TDNNLayer(\n      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (3): TDNNLayer(\n      (kernel): Linear(in_features=512, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (4): TDNNLayer(\n      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n      (activation): ReLU()\n    )\n  )\n  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n  (classifier): Linear(in_features=512, out_features=512, bias=True)\n  (objective): AMSoftmaxLoss(\n    (loss): CrossEntropyLoss()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# # Iterate through the test_loader to get samples\n# for audio, label in test_loader:\n#     # Use audio and label as needed\n#     print(\"audio\", audio.shape)\n#     pass","metadata":{"execution":{"iopub.status.busy":"2024-04-05T12:14:46.915560Z","iopub.status.idle":"2024-04-05T12:14:46.915882Z","shell.execute_reply.started":"2024-04-05T12:14:46.915725Z","shell.execute_reply":"2024-04-05T12:14:46.915739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_curve\n\n# Assuming testLoader provides input tensors and targets\n# Adjust according to your data loading mechanism\nwith torch.no_grad():\n    similarities = []\n    labels = []\n    for inputs, target in testLoader:\n        #print(\"Inputs:\", inputs)\n        #print(\"Target:\", target)\n        if inputs is None or target is None:\n            continue\n            \n        # Extract audio1 and audio2 from inputs\n        audio1 = inputs[:, 0]  \n        audio2 = inputs[:, 1]  \n        \n        # Pass the input tensors to the model\n        audio1 = torch.tensor(audio1)\n        audio2 = torch.tensor(audio2)\n        #print(\"audio1 shape\", audio1.shape)\n        \n        audio1 = audio1.clone().detach().to(device)\n        audio2 = audio2.clone().detach().to(device)\n\n        # Extract embeddings\n        audio1 = feature_extractor(audio1, return_tensors=\"pt\", sampling_rate=16000)\n        audio1 = audio1.input_values.squeeze(0) \n       \n        audio2 = feature_extractor(audio2, return_tensors=\"pt\", sampling_rate=16000)\n        audio2 = audio2.input_values.squeeze(0) \n        \n        # Get embeddings for audio1\n        #print(\"audio1 shape after embedding extraction\", audio1.shape)\n        outputs_audio1 = model(input_values=audio1)\n        embeddings_audio1 = outputs_audio1.embeddings\n        embeddings_audio1 = torch.nn.functional.normalize(embeddings_audio1, dim=-1).cpu()\n        \n        # Get embeddings for audio2\n        \n        outputs_audio2 = model(input_values=audio2)\n        embeddings_audio2 = outputs_audio2.embeddings\n        embeddings_audio2 = torch.nn.functional.normalize(embeddings_audio2, dim=-1).cpu()\n        \n        # Calculate cosine similarity between embeddings\n        similarity = torch.nn.functional.cosine_similarity(embeddings_audio1, embeddings_audio2, dim=-1)\n        \n        print(\"Similarity:\", similarity)\n        print(\"Label:\", target)\n        \n        similarities.extend(similarity.tolist())\n        labels.extend(target.tolist())  # Append the tensor itself, not its item\n\n# Convert lists to numpy arrays\nsimilarities = np.array(similarities)\nlabels = np.array(labels)\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(labels, similarities, pos_label=1)\n\n# Find EER\neer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\neer = (fpr[np.argmin(np.abs(fpr - (1 - tpr)))] + (1 - tpr)[np.argmin(np.abs(fpr - (1 - tpr)))]) / 2\n\nprint(\"Equal Error Rate (EER): {:.2f}%\".format(eer * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:15:16.626840Z","iopub.execute_input":"2024-04-05T15:15:16.627837Z","iopub.status.idle":"2024-04-05T15:15:18.732142Z","shell.execute_reply.started":"2024-04-05T15:15:16.627801Z","shell.execute_reply":"2024-04-05T15:15:18.730573Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3723032804.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audio1 = torch.tensor(audio1)\n/tmp/ipykernel_34/3723032804.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audio2 = torch.tensor(audio2)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m audio2 \u001b[38;5;241m=\u001b[39m audio2\u001b[38;5;241m.\u001b[39minput_values\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Get embeddings for audio1\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#print(\"audio1 shape after embedding extraction\", audio1.shape)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m outputs_audio1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m embeddings_audio1 \u001b[38;5;241m=\u001b[39m outputs_audio1\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m     39\u001b[0m embeddings_audio1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(embeddings_audio1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py:1799\u001b[0m, in \u001b[0;36mWavLMForXVector.forward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1796\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1797\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum \u001b[38;5;28;01melse\u001b[39;00m output_hidden_states\n\u001b[0;32m-> 1799\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwavlm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum:\n\u001b[1;32m   1808\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py:1214\u001b[0m, in \u001b[0;36mWavLMModel.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1209\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1210\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1211\u001b[0m )\n\u001b[1;32m   1212\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1214\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1215\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py:368\u001b[0m, in \u001b[0;36mWavLMFeatureEncoder.forward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    363\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    364\u001b[0m             conv_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    365\u001b[0m             hidden_states,\n\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/wavlm/modeling_wavlm.py:270\u001b[0m, in \u001b[0;36mWavLMGroupNormConvLayer.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 270\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    272\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_states)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"],"ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","output_type":"error"}]},{"cell_type":"markdown","source":"####","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}