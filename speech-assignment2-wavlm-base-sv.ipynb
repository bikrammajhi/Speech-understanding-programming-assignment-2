{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2816656,"sourceType":"datasetVersion","datasetId":1722209},{"sourceId":8028065,"sourceType":"datasetVersion","datasetId":4731558},{"sourceId":8038530,"sourceType":"datasetVersion","datasetId":4739112}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T06:12:36.991270Z","iopub.execute_input":"2024-04-06T06:12:36.992118Z","iopub.status.idle":"2024-04-06T06:12:36.996801Z","shell.execute_reply.started":"2024-04-06T06:12:36.992082Z","shell.execute_reply":"2024-04-06T06:12:36.995862Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-warmup","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:12:38.613391Z","iopub.execute_input":"2024-04-06T06:12:38.614214Z","iopub.status.idle":"2024-04-06T06:12:50.516900Z","shell.execute_reply.started":"2024-04-06T06:12:38.614182Z","shell.execute_reply":"2024-04-06T06:12:50.515743Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-warmup in /opt/conda/lib/python3.10/site-packages (0.1.1)\nRequirement already satisfied: torch>=1.1 in /opt/conda/lib/python3.10/site-packages (from pytorch-warmup) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.1->pytorch-warmup) (2024.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.1->pytorch-warmup) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.1->pytorch-warmup) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import math, torch, torchaudio\nimport soundfile\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os, numpy\nfrom sklearn import metrics\nfrom operator import itemgetter\nimport glob, numpy, os, random, soundfile, torch\nfrom scipy import signal\nimport time\nimport tqdm\nimport pytorch_warmup as warmup\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:13:00.117403Z","iopub.execute_input":"2024-04-06T06:13:00.118324Z","iopub.status.idle":"2024-04-06T06:13:00.124676Z","shell.execute_reply.started":"2024-04-06T06:13:00.118283Z","shell.execute_reply":"2024-04-06T06:13:00.123843Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class test_loader(object):\n    def __init__(self, train_list, train_path, musan_path, rir_path, num_frames, **kwargs):\n        self.train_path = train_path\n        self.num_frames = num_frames\n        self.data_list  = []\n        self.data_label = []\n        lines = open(train_list).read().splitlines()\n        dictkeys = list(set([x.split()[0] for x in lines]))\n        dictkeys.sort()\n        dictkeys = { key : ii for ii, key in enumerate(dictkeys) }\n        for index, line in enumerate(lines):\n            speaker_label = dictkeys[line.split()[0]]\n            file_names = line.split()[1:]  # Get the file names from the line\n            file_paths = []\n            for file_name in file_names:\n                full_file_path = os.path.join(train_path, file_name)\n                file_paths.append(full_file_path)\n            self.data_label.append(speaker_label)\n            self.data_list.append(file_paths)  # Append both file paths\n\n    def __getitem__(self, index):\n        try:\n            # Read the utterance and randomly select the segment\n            audio1, sr1 = soundfile.read(self.data_list[index][0])        \n            length = self.num_frames * 160 + 240\n            if audio1.shape[0] <= length:\n                shortage = length - audio1.shape[0]\n                audio1 = numpy.pad(audio1, (0, shortage), 'wrap')\n            start_frame = numpy.int64(random.random()*(audio1.shape[0]-length))\n            audio1 = audio1[start_frame:start_frame + length]\n            #audio1 = numpy.stack([audio1], axis=0).astype(numpy.float32)\n\n            # Read the second utterance and randomly select the segment\n            audio2, sr2 = soundfile.read(self.data_list[index][1])        \n            if audio2.shape[0] <= length:\n                shortage = length - audio2.shape[0]\n                audio2 = numpy.pad(audio2, (0, shortage), 'wrap')\n            start_frame = numpy.int64(random.random()*(audio2.shape[0]-length))\n            audio2 = audio2[start_frame:start_frame + length]\n            #audio2 = numpy.stack([audio2], axis=0).astype(numpy.float32)\n\n            # Concatenate the two audio files along a new dimension\n            concatenated_audio = numpy.stack([audio1, audio2], axis=0).astype(numpy.float32)\n            # Convert concatenated_audio and label to tensors\n           \n            return torch.tensor(concatenated_audio),(self.data_label[index])\n        \n        except Exception as e:\n#         # If an error occurs (e.g., file not found), print a warning and return None\n#         print(f\"Error processing file {self.data_list[index]}: {e}\")\n            length = self.num_frames * 160 + 240\n            return torch.zeros((2, length)), self.data_label[index]\n        \n\n    def __len__(self):\n        return len(self.data_list)\n\n   ","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:13:18.189397Z","iopub.execute_input":"2024-04-06T06:13:18.189768Z","iopub.status.idle":"2024-04-06T06:13:18.212880Z","shell.execute_reply.started":"2024-04-06T06:13:18.189740Z","shell.execute_reply":"2024-04-06T06:13:18.211787Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import roc_curve\n\n# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n\ntestloader = test_loader(\"/kaggle/input/voxceleb-h/voxceleb_h.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\ntestLoader = DataLoader(testloader, batch_size=128, shuffle=False, num_workers=10) ","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:13:19.827389Z","iopub.execute_input":"2024-04-06T06:13:19.828140Z","iopub.status.idle":"2024-04-06T06:13:23.773883Z","shell.execute_reply.started":"2024-04-06T06:13:19.828104Z","shell.execute_reply":"2024-04-06T06:13:23.773044Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"num_files = len(testloader)\nprint(\"Number of files in testloader:\", num_files)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:12:26.994943Z","iopub.execute_input":"2024-04-06T06:12:26.995314Z","iopub.status.idle":"2024-04-06T06:12:27.000868Z","shell.execute_reply.started":"2024-04-06T06:12:26.995280Z","shell.execute_reply":"2024-04-06T06:12:26.999906Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Number of files in testloader: 552536\n","output_type":"stream"}]},{"cell_type":"code","source":"#testloader = test_loader(\"/kaggle/input/voxceleb-h/voxceleb_h.txt\", \"/kaggle/input/voxceleb1train/wav\", None, None, 300)\niteration_count = 0\nfor batch in testloader:\n    inputs = batch[0]  # Assuming inputs are at index 0\n    target = batch[1]  # Assuming targets are at index 1\n    if inputs is None or target is None:\n        continue\n    # Your code here\n\n   \n    print(\"Iteration:\", iteration_count)\n    iteration_count += 1\n    # Your remaining code here\n\n    print(\"inputs:\", inputs)\n    print(\"Target:\", target)\n    \n    if iteration_count == 10:\n        break\n    \n# Input: tensor([ 0.0028,  0.0027,  0.0027,  ..., -0.0038, -0.0066, -0.0106])\n# Input shape: torch.Size([48240])\n# Target: 1","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:13:31.533461Z","iopub.execute_input":"2024-04-06T06:13:31.534348Z","iopub.status.idle":"2024-04-06T06:13:31.614479Z","shell.execute_reply.started":"2024-04-06T06:13:31.534315Z","shell.execute_reply":"2024-04-06T06:13:31.613471Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Iteration: 0\ninputs: tensor([[ 0.0007, -0.0525, -0.0895,  ...,  0.0495,  0.0458,  0.0446],\n        [-0.1032, -0.0710, -0.0002,  ..., -0.0438, -0.0700, -0.1289]])\nTarget: 1\nIteration: 1\ninputs: tensor([[-0.0132,  0.0008,  0.0080,  ..., -0.0184, -0.0343, -0.0600],\n        [ 0.0128,  0.0123,  0.0106,  ...,  0.0316,  0.0348,  0.0371]])\nTarget: 0\nIteration: 2\ninputs: tensor([[-0.0153, -0.0112, -0.0045,  ...,  0.0022,  0.0274, -0.0385],\n        [-0.0028, -0.0043, -0.0054,  ...,  0.0027,  0.0048,  0.0058]])\nTarget: 1\nIteration: 3\ninputs: tensor([[-0.0013, -0.0018, -0.0014,  ...,  0.0154,  0.0181,  0.0191],\n        [-0.0009, -0.0012, -0.0017,  ...,  0.0009, -0.0008, -0.0016]])\nTarget: 0\nIteration: 4\ninputs: tensor([[-0.0367, -0.0519, -0.0566,  ...,  0.0062,  0.0053,  0.0048],\n        [-0.0208, -0.0199, -0.0186,  ...,  0.0053,  0.0101,  0.0155]])\nTarget: 1\nIteration: 5\ninputs: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\nTarget: 0\nIteration: 6\ninputs: tensor([[-0.0034, -0.0017, -0.0016,  ..., -0.0719, -0.0543, -0.0353],\n        [-0.0083, -0.0048, -0.0022,  ...,  0.0018, -0.0036, -0.0133]])\nTarget: 1\nIteration: 7\ninputs: tensor([[-0.0275,  0.0127,  0.0057,  ..., -0.0104, -0.0118, -0.0159],\n        [ 0.0600,  0.0502,  0.0354,  ..., -0.0155, -0.0184, -0.0237]])\nTarget: 0\nIteration: 8\ninputs: tensor([[ 1.6846e-02,  1.6602e-02,  1.6479e-02,  ..., -9.0027e-03,\n          5.2795e-03,  7.5989e-03],\n        [ 3.0518e-04,  4.2725e-04, -3.0518e-05,  ...,  2.2888e-03,\n         -1.0162e-02,  1.6724e-02]])\nTarget: 1\nIteration: 9\ninputs: tensor([[ 0.0564,  0.0532,  0.0510,  ..., -0.0063, -0.0149,  0.0106],\n        [ 0.0210,  0.0134,  0.0083,  ..., -0.0029,  0.0074, -0.0044]])\nTarget: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"## Model finetuned on voxceleb1 dataset\n\n#microsoft/unispeech-sat-base-sv\n#microsoft/wavlm-base-sv\n#microsoft/wavlm-base-plus-sv","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:12:27.072880Z","iopub.execute_input":"2024-04-06T06:12:27.073135Z","iopub.status.idle":"2024-04-06T06:12:27.076487Z","shell.execute_reply.started":"2024-04-06T06:12:27.073114Z","shell.execute_reply":"2024-04-06T06:12:27.075852Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_curve\n\nfrom transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n\nfeature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-sv')\nmodel = WavLMForXVector.from_pretrained('microsoft/wavlm-base-sv')\n\n\n# Move model and feature extractor to CUDA if available\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:13:36.688252Z","iopub.execute_input":"2024-04-06T06:13:36.688859Z","iopub.status.idle":"2024-04-06T06:13:37.601386Z","shell.execute_reply.started":"2024-04-06T06:13:36.688826Z","shell.execute_reply":"2024-04-06T06:13:37.600445Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/wavlm-base-sv were not used when initializing WavLMForXVector: ['wavlm.encoder.pos_conv_embed.conv.weight_g', 'wavlm.encoder.pos_conv_embed.conv.weight_v']\n- This IS expected if you are initializing WavLMForXVector from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing WavLMForXVector from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of WavLMForXVector were not initialized from the model checkpoint at microsoft/wavlm-base-sv and are newly initialized: ['wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wavlm.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"WavLMForXVector(\n  (wavlm): WavLMModel(\n    (feature_extractor): WavLMFeatureEncoder(\n      (conv_layers): ModuleList(\n        (0): WavLMGroupNormConvLayer(\n          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n          (activation): GELUActivation()\n          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n        )\n        (1-4): 4 x WavLMNoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n        (5-6): 2 x WavLMNoLayerNormConvLayer(\n          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n          (activation): GELUActivation()\n        )\n      )\n    )\n    (feature_projection): WavLMFeatureProjection(\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (projection): Linear(in_features=512, out_features=768, bias=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): WavLMEncoder(\n      (pos_conv_embed): WavLMPositionalConvEmbedding(\n        (conv): ParametrizedConv1d(\n          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n          (parametrizations): ModuleDict(\n            (weight): ParametrizationList(\n              (0): _WeightNorm()\n            )\n          )\n        )\n        (padding): WavLMSamePadLayer()\n        (activation): GELUActivation()\n      )\n      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layers): ModuleList(\n        (0): WavLMEncoderLayer(\n          (attention): WavLMAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n            (rel_attn_embed): Embedding(320, 12)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): WavLMFeedForward(\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1-11): 11 x WavLMEncoderLayer(\n          (attention): WavLMAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            (gru_rel_pos_linear): Linear(in_features=64, out_features=8, bias=True)\n          )\n          (dropout): Dropout(p=0.1, inplace=False)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (feed_forward): WavLMFeedForward(\n            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n            (output_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (projector): Linear(in_features=768, out_features=512, bias=True)\n  (tdnn): ModuleList(\n    (0): TDNNLayer(\n      (kernel): Linear(in_features=2560, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (1-2): 2 x TDNNLayer(\n      (kernel): Linear(in_features=1536, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (3): TDNNLayer(\n      (kernel): Linear(in_features=512, out_features=512, bias=True)\n      (activation): ReLU()\n    )\n    (4): TDNNLayer(\n      (kernel): Linear(in_features=512, out_features=1500, bias=True)\n      (activation): ReLU()\n    )\n  )\n  (feature_extractor): Linear(in_features=3000, out_features=512, bias=True)\n  (classifier): Linear(in_features=512, out_features=512, bias=True)\n  (objective): AMSoftmaxLoss(\n    (loss): CrossEntropyLoss()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.metrics import roc_curve\n\nwith torch.no_grad():\n    similarities = []\n    labels = []\n    for inputs, target in testLoader:\n        #print(\"Inputs:\", inputs)\n        #print(\"Target:\", target)\n        if inputs is None or target is None:\n            continue\n            \n        # Extract audio1 and audio2 from inputs\n        audio1 = inputs[:, 0]  \n        audio2 = inputs[:, 1]  \n        \n        # Pass the input tensors to the model\n        audio1 = torch.tensor(audio1)\n        audio2 = torch.tensor(audio2)\n        #print(\"audio1 shape\", audio1.shape)\n        \n        # Extract embeddings\n        audio1 = feature_extractor(audio1, return_tensors=\"pt\", sampling_rate=16000)\n        audio1 = audio1.input_values.squeeze(0) \n       \n        audio2 = feature_extractor(audio2, return_tensors=\"pt\", sampling_rate=16000)\n        audio2 = audio2.input_values.squeeze(0) \n        \n        audio1 = audio1.clone().detach().to(device)\n        audio2 = audio2.clone().detach().to(device)\n        \n        # Get embeddings for audio1\n        #print(\"audio1 shape after embedding extraction\", audio1.shape)\n        outputs_audio1 = model(input_values=audio1)\n        embeddings_audio1 = outputs_audio1.embeddings\n        embeddings_audio1 = torch.nn.functional.normalize(embeddings_audio1, dim=-1).cpu()\n        \n        # Get embeddings for audio2\n        outputs_audio2 = model(input_values=audio2)\n        embeddings_audio2 = outputs_audio2.embeddings\n        embeddings_audio2 = torch.nn.functional.normalize(embeddings_audio2, dim=-1).cpu()\n        \n        # Calculate cosine similarity between embeddings\n        similarity = torch.nn.functional.cosine_similarity(embeddings_audio1, embeddings_audio2, dim=-1)\n        \n#         print(\"Similarity:\", similarity)\n#         print(\"Label:\", target)\n        \n        similarities.extend(similarity.tolist())\n        labels.extend(target.tolist())  # Append the tensor itself, not its item\n\n# Convert lists to numpy arrays\nsimilarities = np.array(similarities)\nlabels = np.array(labels)\n\n# Calculate ROC curve\nfpr, tpr, thresholds = roc_curve(labels, similarities, pos_label=1)\n\n# Find EER\neer_threshold = thresholds[np.argmin(np.abs(fpr - (1 - tpr)))]\neer = (fpr[np.argmin(np.abs(fpr - (1 - tpr)))] + (1 - tpr)[np.argmin(np.abs(fpr - (1 - tpr)))]) / 2\n\nprint(\"Equal Error Rate (EER): {:.2f}%\".format(eer * 100))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-06T06:14:00.417337Z","iopub.execute_input":"2024-04-06T06:14:00.418002Z","iopub.status.idle":"2024-04-06T08:28:24.863770Z","shell.execute_reply.started":"2024-04-06T06:14:00.417972Z","shell.execute_reply":"2024-04-06T08:28:24.861608Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1011509741.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audio1 = torch.tensor(audio1)\n/tmp/ipykernel_34/1011509741.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  audio2 = torch.tensor(audio2)\n","output_type":"stream"},{"name":"stdout","text":"Equal Error Rate (EER): 14.85%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"####","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}